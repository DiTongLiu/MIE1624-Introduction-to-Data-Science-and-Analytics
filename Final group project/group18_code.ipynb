{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "group18_code.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soviet-religious"
      },
      "source": [
        "Below is all packages which we used for this project"
      ],
      "id": "soviet-religious"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vietnamese-ottawa"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import sparse\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from scipy.sparse import hstack\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.utils import np_utils\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import csv\n",
        "from collections import defaultdict\n",
        "from sklearn.linear_model import LinearRegression\n",
        "# nltk.download('stopwords')\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('wordnet')"
      ],
      "id": "vietnamese-ottawa",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2a07Hn1dIjP",
        "outputId": "ebdf8e8b-6ba5-4719-a327-7b7e774ba563"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "id": "K2a07Hn1dIjP",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imported-kentucky"
      },
      "source": [
        "Below is the code to read both train.csv file and test.csv file and stored in dataframe df1(trian.csv) and df2(test.csv)"
      ],
      "id": "imported-kentucky"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "judicial-drunk"
      },
      "source": [
        "#reading the corresponding csv file\n",
        "df1 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/mie1624/train.csv')\n",
        "df2 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/mie1624/test.csv')"
      ],
      "id": "judicial-drunk",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35f1t1VOehx-"
      },
      "source": [
        ""
      ],
      "id": "35f1t1VOehx-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "civic-stretch"
      },
      "source": [
        "Below is the code for us to apply data cleaning. We only use the reviewText and summary column as selected features. For train data's reviewText and summary column cleaning, we use the regular expression modle to remove all special character, change all characters to lower case and use nltk stopwords libraray to remove stop words. Then we applied tf-idf to extract 1500 features and make as a matrix."
      ],
      "id": "civic-stretch"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "breathing-offense"
      },
      "source": [
        "# train data cleaning\n",
        "\n",
        "df_train = df1.drop(columns = ['image', 'unixReviewTime','reviewHash'])\n",
        "df_train = df_train.dropna()\n",
        "\n",
        "# Defining a function to clean reviews\n",
        "def dataCleaning(tweetList):\n",
        "    \n",
        "    # remove all special characters\n",
        "    regex = re.compile('[^a-zA-Z0-9 ]')\n",
        "    tweetList1 = [regex.sub('', tw) for tw in tweetList]\n",
        "    \n",
        "    # change all characters to lowercase\n",
        "    tweetList1 = [tw.lower() for tw in tweetList1]\n",
        "        \n",
        "    # remove stop words using nltk stopwords library\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    for i in range(len(tweetList1)):\n",
        "        example_sent = tweetList1[i]\n",
        "        word_tokens = word_tokenize(example_sent)\n",
        "        filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
        "        filtered_sentence = []\n",
        "        for w in word_tokens:\n",
        "            if w not in stop_words:\n",
        "                filtered_sentence.append(w)\n",
        "        tweetList1[i] = ' '.join(filtered_sentence)\n",
        "        \n",
        "    wn = nltk.WordNetLemmatizer()\n",
        "    for i in range(len(tweetList1)):\n",
        "        w_list = [wn.lemmatize(word) for word in tweetList1[i].split()]\n",
        "        tweetList1[i] = \" \".join(w_list)\n",
        "        \n",
        "    return(tweetList1)\n",
        "\n",
        "allRatings = []\n",
        "userRatings = defaultdict(list)\n",
        "itemRatings = defaultdict(list)\n",
        "\n",
        "data = df1\n",
        "\n",
        "allRatings = data['overall'].to_list()\n",
        "\n",
        "for index_ in data.index:\n",
        "    rating = data.loc[index_,'overall']\n",
        "    userID = data.loc[index_,'reviewerID']\n",
        "    userRatings[userID].append(rating)\n",
        "    \n",
        "    itemID = data.loc[index_,'itemID']\n",
        "    itemRatings[itemID].append(rating)\n",
        "\n",
        "globalAverage = sum(allRatings)/len(allRatings)\n",
        "\n",
        "userAverage = {}\n",
        "for u in userRatings:\n",
        "    userAverage[u] = sum(userRatings[u]) / len(userRatings[u])\n",
        "itemAverage = {}\n",
        "for i in itemRatings:\n",
        "    itemAverage[i] = sum(itemRatings[i]) / len(itemRatings[i])\n",
        "\n",
        "user_avg_array_train = []\n",
        "for l in df_train['reviewerID'].tolist():\n",
        "    user_avg_array_train.append(userAverage[l])\n",
        "item_avg_array_train = []\n",
        "for l in df_train['itemID'].tolist():\n",
        "    item_avg_array_train.append(itemAverage[l])\n",
        "     \n",
        "summary_train = dataCleaning(df_train['summary'].tolist())\n",
        "df_train['summary'] = summary_train\n",
        "review_train = dataCleaning(df_train['reviewText'].tolist())\n",
        "df_train['reviewText'] = review_train\n",
        "\n",
        "# One-hot encode category columns\n",
        "category_array_train = pd.get_dummies(df_train[['category']], prefix_sep='_', drop_first=True).values\n",
        "\n",
        "text_train = df_train[\"reviewText\"] + ' ' + df_train[\"summary\"]\n",
        "\n",
        "#Using the Tfidf Vectorizer on the summary and review\n",
        "vectorizer = TfidfVectorizer(max_features=2000)\n",
        "fit_vectorizer = vectorizer.fit(text_train.tolist())\n",
        "tfidf_train = fit_vectorizer.transform(text_train.tolist())"
      ],
      "id": "breathing-offense",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "romantic-manor"
      },
      "source": [
        "Below is the code for us to apply data cleaning. We only use the reviewText and summary column as selected features. For test data's reviewText and summary column cleaning, we use the regular expression modle to remove all special character, change all characters to lower case and use nltk stopwords libraray to remove stop words. Then we applied tf-idf to extract 1500 features and make as a matrix."
      ],
      "id": "romantic-manor"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "perfect-stuff"
      },
      "source": [
        "# test data cleaning\n",
        "\n",
        "df_test = df2.drop(columns = ['image', 'unixReviewTime','reviewHash'])\n",
        "\n",
        "for column in df_test.columns:\n",
        "    df_test[column].fillna(df_test[column].mode()[0], inplace=True)\n",
        "    \n",
        "user_avg_array_test = []\n",
        "for l in df_test['reviewerID'].tolist():\n",
        "    user_avg_array_test.append(userAverage[l])\n",
        "item_avg_array_test = []\n",
        "for l in df_test['itemID'].tolist():\n",
        "    if l in itemAverage:\n",
        "        item_avg_array_test.append(itemAverage[l])\n",
        "    else:\n",
        "        item_avg_array_test.append(globalAverage)\n",
        "\n",
        "summary_test = dataCleaning(df_test['summary'].tolist())\n",
        "df_test['summary'] = summary_test\n",
        "review_test = dataCleaning(df_test['reviewText'].tolist())\n",
        "df_test['reviewText'] = review_test\n",
        "\n",
        "# One-hot encode category columns\n",
        "category_array_test = pd.get_dummies(df_test[['category']], prefix_sep='_', drop_first=True).values\n",
        "\n",
        "text_test = df_test[\"reviewText\"] + ' ' + df_test[\"summary\"]\n",
        "\n",
        "tfidf_test = fit_vectorizer.transform(text_test.tolist())"
      ],
      "id": "perfect-stuff",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "complex-commercial"
      },
      "source": [
        "Below we convert the tf-idf result based on both training dataset and testing dataset to array. Then asssigned the corresponding array to X_train and X_test. The y_train dataset will be the 'overall' column from the training dataset."
      ],
      "id": "complex-commercial"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "certain-french"
      },
      "source": [
        "col1_train = tfidf_train\n",
        "col2_train = np.asarray(user_avg_array_train).reshape(-1,1)\n",
        "col3_train = np.asarray(item_avg_array_train).reshape(-1,1)\n",
        "col4_train = category_array_train\n",
        "# clean_matrix_train = hstack((col1_train, col2_train, col3_train, col4_train))\n",
        "clean_matrix_train  = col1_train.toarray()\n",
        "\n",
        "col1_test = tfidf_test\n",
        "col2_test = np.asarray(user_avg_array_test).reshape(-1,1)\n",
        "col3_test = np.asarray(item_avg_array_test).reshape(-1,1)\n",
        "col4_test = category_array_test\n",
        "# clean_matrix_test = hstack((col1_test, col2_test, col3_test, col4_test))\n",
        "clean_matrix_test = col1_test.toarray()\n",
        "\n",
        "# scaler = StandardScaler()\n",
        "# X_train = scaler.fit_transform(clean_matrix_train.todense())\n",
        "# X_test = scaler.fit_transform(clean_matrix_test.todense())\n",
        "X_train = clean_matrix_train\n",
        "X_test = clean_matrix_test\n",
        "y_train = df_train['overall'].astype('int64')"
      ],
      "id": "certain-french",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "passing-presentation"
      },
      "source": [
        "We use linear regression for the first model to get trained. We first use the cross_val_score to find the mse for this model which is showing on the next cell. "
      ],
      "id": "passing-presentation"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "robust-heater",
        "outputId": "b27035be-2c2e-4435-9790-dcdd75686e5c"
      },
      "source": [
        "print('linear regression:')\n",
        "for param in [1]:\n",
        "    model = LinearRegression()\n",
        "    scores = cross_val_score(model, X_train, y_train, scoring='neg_mean_squared_error', cv=5)\n",
        "    mean_mse = scores.mean()\n",
        "    print(\"mse:\", mean_mse*-1)"
      ],
      "id": "robust-heater",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "linear regression:\n",
            "mse: 0.5725764961062434\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNDGwXRmkSuz"
      },
      "source": [
        "We used multinomial logistic regression from Sklearn for our second model. We performed hyperparameter tuning of the C parameter with 2-fold cross validation. The best logistic regression model, using a C value of 1, gives a training MSE value of 0.7372."
      ],
      "id": "FNDGwXRmkSuz"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4v79KqUj47w",
        "outputId": "129cc307-d732-4c2f-9336-7e5d80e7d2c2"
      },
      "source": [
        "print('logistic regression:')\n",
        "for param in [0.1,1]:\n",
        "    model = LogisticRegression(C=param)\n",
        "    scores = cross_val_score(model, X_train, y_train, scoring='neg_mean_squared_error', cv=2)\n",
        "    mean_mse = scores.mean()\n",
        "    print('C =', param, \", mse:\", mean_mse*-1)"
      ],
      "id": "N4v79KqUj47w",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "logistic regression:\n",
            "C = 0.1 , mse: 0.9843080036113085\n",
            "C = 1 , mse: 0.7372239937031677\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SIozsQurMc4"
      },
      "source": [
        "We used multinomial Naive-Bayes from Sklearn for our third model. We performed hyperparameter tuning of the alpha parameter with 5-fold cross validation. The best Naive-Bayes model, using an alpha value of 0.0001, gives a training MSE value of 1.1891."
      ],
      "id": "6SIozsQurMc4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LE8Ep1VBrZyB",
        "outputId": "cbab9828-befa-4c11-fb5a-17a8ce77bab4"
      },
      "source": [
        "print('multinomial Naive Bayes:')\n",
        "for param in [0.0001,0.001,0.01,0.1,1]:\n",
        "    model = MultinomialNB(alpha=param)\n",
        "    scores = cross_val_score(model, X_train, y_train, scoring='neg_mean_squared_error', cv=5)\n",
        "    mean_mse = scores.mean()\n",
        "    print('alpha =', param, \", mse:\", mean_mse*-1)"
      ],
      "id": "LE8Ep1VBrZyB",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "multinomial Naive Bayes:\n",
            "alpha = 0.0001 , mse: 1.1890776292309737\n",
            "alpha = 0.001 , mse: 1.1890776292309737\n",
            "alpha = 0.01 , mse: 1.1892910352557746\n",
            "alpha = 0.1 , mse: 1.1906981769718807\n",
            "alpha = 1 , mse: 1.2022354311952843\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eUpDEFbsUse"
      },
      "source": [
        "We used a 3-layered neural network from Keras for our fourth and last model. We used Rectified Linear Unit as the activation function for the hidden layer of model, and for the activation function of the output layer we used the Softmax function. We set our loss function to be categorical cross-entropy. For the model training process we used a batch size of 256 and 50 epochs. The results can be seen below>"
      ],
      "id": "5eUpDEFbsUse"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUYwEPCHshiO"
      },
      "source": [
        "encoder_nn = LabelEncoder()\n",
        "encoder_nn.fit(y_train)\n",
        "encoded_Y = encoder_nn.transform(y_train)\n",
        "y_train_nn = np_utils.to_categorical(encoded_Y)"
      ],
      "id": "CUYwEPCHshiO",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CBCVeNntO_W",
        "outputId": "abb8647e-7a8d-4dae-83ef-07cbacd8c19f"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(8, input_dim=2000, activation='relu'))\n",
        "model.add(Dense(5, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['mse','accuracy'])\n",
        "model.fit(X_train, y_train_nn, epochs=50, batch_size=256)"
      ],
      "id": "2CBCVeNntO_W",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "586/586 [==============================] - 3s 4ms/step - loss: 1.2133 - mse: 0.1231 - accuracy: 0.6327\n",
            "Epoch 2/50\n",
            "586/586 [==============================] - 2s 4ms/step - loss: 0.7434 - mse: 0.0770 - accuracy: 0.7119\n",
            "Epoch 3/50\n",
            "586/586 [==============================] - 2s 4ms/step - loss: 0.6755 - mse: 0.0718 - accuracy: 0.7324\n",
            "Epoch 4/50\n",
            "586/586 [==============================] - 2s 4ms/step - loss: 0.6602 - mse: 0.0707 - accuracy: 0.7386\n",
            "Epoch 5/50\n",
            "586/586 [==============================] - 2s 4ms/step - loss: 0.6485 - mse: 0.0698 - accuracy: 0.7427\n",
            "Epoch 6/50\n",
            "586/586 [==============================] - 2s 4ms/step - loss: 0.6481 - mse: 0.0697 - accuracy: 0.7437\n",
            "Epoch 7/50\n",
            "586/586 [==============================] - 2s 4ms/step - loss: 0.6463 - mse: 0.0696 - accuracy: 0.7438\n",
            "Epoch 8/50\n",
            "586/586 [==============================] - 2s 4ms/step - loss: 0.6393 - mse: 0.0690 - accuracy: 0.7460\n",
            "Epoch 9/50\n",
            "586/586 [==============================] - 2s 4ms/step - loss: 0.6428 - mse: 0.0693 - accuracy: 0.7447\n",
            "Epoch 10/50\n",
            "586/586 [==============================] - 2s 4ms/step - loss: 0.6382 - mse: 0.0690 - accuracy: 0.7456\n",
            "Epoch 11/50\n",
            "586/586 [==============================] - 2s 4ms/step - loss: 0.6363 - mse: 0.0687 - accuracy: 0.7480\n",
            "Epoch 12/50\n",
            "586/586 [==============================] - 3s 4ms/step - loss: 0.6358 - mse: 0.0687 - accuracy: 0.7480\n",
            "Epoch 13/50\n",
            "586/586 [==============================] - 2s 4ms/step - loss: 0.6331 - mse: 0.0685 - accuracy: 0.7486\n",
            "Epoch 14/50\n",
            "586/586 [==============================] - 3s 4ms/step - loss: 0.6312 - mse: 0.0683 - accuracy: 0.7483\n",
            "Epoch 15/50\n",
            "586/586 [==============================] - 3s 5ms/step - loss: 0.6293 - mse: 0.0681 - accuracy: 0.7505\n",
            "Epoch 16/50\n",
            "586/586 [==============================] - 3s 5ms/step - loss: 0.6324 - mse: 0.0684 - accuracy: 0.7487\n",
            "Epoch 17/50\n",
            "586/586 [==============================] - 2s 4ms/step - loss: 0.6305 - mse: 0.0682 - accuracy: 0.7500\n",
            "Epoch 18/50\n",
            "586/586 [==============================] - 2s 4ms/step - loss: 0.6283 - mse: 0.0679 - accuracy: 0.7516\n",
            "Epoch 19/50\n",
            "586/586 [==============================] - 3s 5ms/step - loss: 0.6311 - mse: 0.0682 - accuracy: 0.7494\n",
            "Epoch 20/50\n",
            "586/586 [==============================] - 3s 5ms/step - loss: 0.6289 - mse: 0.0681 - accuracy: 0.7499\n",
            "Epoch 21/50\n",
            "586/586 [==============================] - 3s 5ms/step - loss: 0.6291 - mse: 0.0681 - accuracy: 0.7492\n",
            "Epoch 22/50\n",
            "586/586 [==============================] - 2s 4ms/step - loss: 0.6275 - mse: 0.0680 - accuracy: 0.7500\n",
            "Epoch 23/50\n",
            "586/586 [==============================] - 2s 4ms/step - loss: 0.6258 - mse: 0.0676 - accuracy: 0.7524\n",
            "Epoch 24/50\n",
            "586/586 [==============================] - 2s 4ms/step - loss: 0.6285 - mse: 0.0681 - accuracy: 0.7506\n",
            "Epoch 25/50\n",
            "586/586 [==============================] - 2s 4ms/step - loss: 0.6219 - mse: 0.0675 - accuracy: 0.7526\n",
            "Epoch 26/50\n",
            "586/586 [==============================] - 2s 4ms/step - loss: 0.6228 - mse: 0.0677 - accuracy: 0.7519\n",
            "Epoch 27/50\n",
            "586/586 [==============================] - 2s 4ms/step - loss: 0.6214 - mse: 0.0674 - accuracy: 0.7519\n",
            "Epoch 28/50\n",
            "586/586 [==============================] - 2s 4ms/step - loss: 0.6184 - mse: 0.0671 - accuracy: 0.7534\n",
            "Epoch 29/50\n",
            "586/586 [==============================] - 2s 4ms/step - loss: 0.6221 - mse: 0.0675 - accuracy: 0.7518\n",
            "Epoch 30/50\n",
            "586/586 [==============================] - 2s 4ms/step - loss: 0.6251 - mse: 0.0677 - accuracy: 0.7527\n",
            "Epoch 31/50\n",
            "586/586 [==============================] - 2s 4ms/step - loss: 0.6227 - mse: 0.0675 - accuracy: 0.7526\n",
            "Epoch 32/50\n",
            "586/586 [==============================] - 2s 4ms/step - loss: 0.6150 - mse: 0.0668 - accuracy: 0.7555\n",
            "Epoch 33/50\n",
            "586/586 [==============================] - 2s 4ms/step - loss: 0.6197 - mse: 0.0672 - accuracy: 0.7534\n",
            "Epoch 34/50\n",
            "586/586 [==============================] - 3s 5ms/step - loss: 0.6164 - mse: 0.0669 - accuracy: 0.7546\n",
            "Epoch 35/50\n",
            "586/586 [==============================] - 2s 4ms/step - loss: 0.6181 - mse: 0.0671 - accuracy: 0.7534\n",
            "Epoch 36/50\n",
            "586/586 [==============================] - 2s 4ms/step - loss: 0.6158 - mse: 0.0669 - accuracy: 0.7553\n",
            "Epoch 37/50\n",
            "586/586 [==============================] - 2s 4ms/step - loss: 0.6175 - mse: 0.0671 - accuracy: 0.7531\n",
            "Epoch 38/50\n",
            "586/586 [==============================] - 2s 4ms/step - loss: 0.6159 - mse: 0.0668 - accuracy: 0.7550\n",
            "Epoch 39/50\n",
            "586/586 [==============================] - 2s 4ms/step - loss: 0.6147 - mse: 0.0666 - accuracy: 0.7567\n",
            "Epoch 40/50\n",
            "586/586 [==============================] - 2s 4ms/step - loss: 0.6148 - mse: 0.0666 - accuracy: 0.7565\n",
            "Epoch 41/50\n",
            "586/586 [==============================] - 2s 4ms/step - loss: 0.6136 - mse: 0.0667 - accuracy: 0.7542\n",
            "Epoch 42/50\n",
            "586/586 [==============================] - 2s 4ms/step - loss: 0.6152 - mse: 0.0669 - accuracy: 0.7549\n",
            "Epoch 43/50\n",
            "586/586 [==============================] - 3s 4ms/step - loss: 0.6141 - mse: 0.0667 - accuracy: 0.7553\n",
            "Epoch 44/50\n",
            "586/586 [==============================] - 3s 4ms/step - loss: 0.6135 - mse: 0.0666 - accuracy: 0.7562\n",
            "Epoch 45/50\n",
            "586/586 [==============================] - 3s 4ms/step - loss: 0.6101 - mse: 0.0662 - accuracy: 0.7576\n",
            "Epoch 46/50\n",
            "586/586 [==============================] - 3s 4ms/step - loss: 0.6120 - mse: 0.0664 - accuracy: 0.7575\n",
            "Epoch 47/50\n",
            "586/586 [==============================] - 3s 4ms/step - loss: 0.6117 - mse: 0.0664 - accuracy: 0.7561\n",
            "Epoch 48/50\n",
            "586/586 [==============================] - 2s 4ms/step - loss: 0.6048 - mse: 0.0658 - accuracy: 0.7589\n",
            "Epoch 49/50\n",
            "586/586 [==============================] - 3s 4ms/step - loss: 0.6063 - mse: 0.0659 - accuracy: 0.7590\n",
            "Epoch 50/50\n",
            "586/586 [==============================] - 3s 4ms/step - loss: 0.6123 - mse: 0.0666 - accuracy: 0.7556\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fabb175a550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "floating-school"
      },
      "source": [
        "Since the lowest MSE value we got was from the linear regression model (around 0.58367), we will apply this model to the testing dataset to find the prediction. We name the prediction array as the lr_predictions and stored predictions in this list."
      ],
      "id": "floating-school"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "entire-strand"
      },
      "source": [
        "lr_best_model = LinearRegression()\n",
        "lr_best_model_fit = lr_best_model.fit(X_train, y_train)\n",
        "lr_predictions = lr_best_model_fit.predict(X_test)"
      ],
      "id": "entire-strand",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "developmental-karaoke",
        "outputId": "8f6c8d11-b69c-491a-ac77-4488ae1f3d2a"
      },
      "source": [
        "lr_predictions"
      ],
      "id": "developmental-karaoke",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4.229565  , 4.68056887, 4.54808571, ..., 4.52654549, 4.58256319,\n",
              "       3.8346842 ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suitable-deadline"
      },
      "source": [
        "Below is the code that we implemented to write the linear regression predictions to a pre-defined csv file named as test_result.csv. Then we can manually copy the predictions to the rating_pairs.csv file that we are going to submit on the kaggle. "
      ],
      "id": "suitable-deadline"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sunset-confidentiality"
      },
      "source": [
        "# outfile = open('test_result.csv','w')\n",
        "# out = csv.writer(outfile)\n",
        "# out.writerows(map(lambda x: [x], lr_predictions))\n",
        "# outfile.close()"
      ],
      "id": "sunset-confidentiality",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "musical-ocean"
      },
      "source": [
        ""
      ],
      "id": "musical-ocean",
      "execution_count": null,
      "outputs": []
    }
  ]
}