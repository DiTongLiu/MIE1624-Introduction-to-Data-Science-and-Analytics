{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97dcc747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "243cf3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nlp\n",
    "# !pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1ca47b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "import matplotlib.pyplot as plt\n",
    "import nlp\n",
    "import random\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc5b9c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the corresponding csv file\n",
    "df1 = pd.read_csv('train.csv')\n",
    "df2 = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f64dae4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data cleaning\n",
    "\n",
    "df_train = df1[['reviewText','summary','overall']]\n",
    "df_train = df_train.dropna()\n",
    "\n",
    "# Defining a function to clean reviews\n",
    "def dataCleaning(tweetList):\n",
    "    \n",
    "    # remove all special characters\n",
    "    regex = re.compile('[^a-zA-Z0-9 ]')\n",
    "    tweetList1 = [regex.sub('', tw) for tw in tweetList]\n",
    "    \n",
    "    # change all characters to lowercase\n",
    "    tweetList1 = [tw.lower() for tw in tweetList1]\n",
    "        \n",
    "    # remove stop words using nltk stopwords library\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    for i in range(len(tweetList1)):\n",
    "        example_sent = tweetList1[i]\n",
    "        word_tokens = word_tokenize(example_sent)\n",
    "        filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "        filtered_sentence = []\n",
    "        for w in word_tokens:\n",
    "            if w not in stop_words:\n",
    "                filtered_sentence.append(w)\n",
    "        tweetList1[i] = ' '.join(filtered_sentence)\n",
    "        \n",
    "    wn = nltk.WordNetLemmatizer()\n",
    "    for i in range(len(tweetList1)):\n",
    "        w_list = [wn.lemmatize(word) for word in tweetList1[i].split()]\n",
    "        tweetList1[i] = \" \".join(w_list)\n",
    "        \n",
    "    return(tweetList1)\n",
    "\n",
    "# allRatings = []\n",
    "# userRatings = defaultdict(list)\n",
    "# itemRatings = defaultdict(list)\n",
    "\n",
    "# data = df1\n",
    "\n",
    "# allRatings = data['overall'].to_list()\n",
    "\n",
    "# for index_ in data.index:\n",
    "#     rating = data.loc[index_,'overall']\n",
    "#     userID = data.loc[index_,'reviewerID']\n",
    "#     userRatings[userID].append(rating)\n",
    "    \n",
    "#     itemID = data.loc[index_,'itemID']\n",
    "#     itemRatings[itemID].append(rating)\n",
    "\n",
    "# globalAverage = sum(allRatings)/len(allRatings)\n",
    "\n",
    "# userAverage = {}\n",
    "# for u in userRatings:\n",
    "#     userAverage[u] = sum(userRatings[u]) / len(userRatings[u])\n",
    "# itemAverage = {}\n",
    "# for i in itemRatings:\n",
    "#     itemAverage[i] = sum(itemRatings[i]) / len(itemRatings[i])\n",
    "\n",
    "# user_avg_array_train = []\n",
    "# for l in df_train['reviewerID'].tolist():\n",
    "#     user_avg_array_train.append(userAverage[l])\n",
    "# item_avg_array_train = []\n",
    "# for l in df_train['itemID'].tolist():\n",
    "#     item_avg_array_train.append(itemAverage[l])\n",
    "     \n",
    "summary_train = dataCleaning(df_train['summary'].tolist())\n",
    "df_train['summary'] = summary_train\n",
    "review_train = dataCleaning(df_train['reviewText'].tolist())\n",
    "df_train['reviewText'] = review_train\n",
    "\n",
    "# # One-hot encode category columns\n",
    "# category_array_train = pd.get_dummies(df_train[['category']], prefix_sep='_', drop_first=True).values\n",
    "\n",
    "# #Using the Tfidf Vectorizer on the summary and review\n",
    "# vectorizer = TfidfVectorizer(max_features=2000)\n",
    "# fit_vectorizer = vectorizer.fit(text_train.tolist())\n",
    "# tfidf_train = fit_vectorizer.transform(text_train.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "057a21d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data cleaning\n",
    "\n",
    "df_test = df2[['reviewText','summary']]\n",
    "\n",
    "for column in df_test.columns:\n",
    "    df_test[column].fillna(df_test[column].mode()[0], inplace=True)\n",
    "    \n",
    "# user_avg_array_test = []\n",
    "# for l in df_test['reviewerID'].tolist():\n",
    "#     user_avg_array_test.append(userAverage[l])\n",
    "# item_avg_array_test = []\n",
    "# for l in df_test['itemID'].tolist():\n",
    "#     if l in itemAverage:\n",
    "#         item_avg_array_test.append(itemAverage[l])\n",
    "#     else:\n",
    "#         item_avg_array_test.append(globalAverage)\n",
    "\n",
    "summary_test = dataCleaning(df_test['summary'].tolist())\n",
    "df_test['summary'] = summary_test\n",
    "review_test = dataCleaning(df_test['reviewText'].tolist())\n",
    "df_test['reviewText'] = review_test\n",
    "\n",
    "# One-hot encode category columns\n",
    "# category_array_test = pd.get_dummies(df_test[['category']], prefix_sep='_', drop_first=True).values\n",
    "\n",
    "text_test = df_test[\"reviewText\"] + ' ' + df_test[\"summary\"]\n",
    "\n",
    "# tfidf_test = fit_vectorizer.transform(text_test.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd92f216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# col1_train = tfidf_train\n",
    "# col2_train = np.asarray(user_avg_array_train).reshape(-1,1)\n",
    "# col3_train = np.asarray(item_avg_array_train).reshape(-1,1)\n",
    "# col4_train = category_array_train\n",
    "# # clean_matrix_train = hstack((col1_train, col2_train, col3_train, col4_train))\n",
    "# clean_matrix_train  = col1_train.toarray()\n",
    "\n",
    "# col1_test = tfidf_test\n",
    "# col2_test = np.asarray(user_avg_array_test).reshape(-1,1)\n",
    "# col3_test = np.asarray(item_avg_array_test).reshape(-1,1)\n",
    "# col4_test = category_array_test\n",
    "# # clean_matrix_test = hstack((col1_test, col2_test, col3_test, col4_test))\n",
    "# clean_matrix_test = col1_test.toarray()\n",
    "\n",
    "# # scaler = StandardScaler()\n",
    "# # X_train = scaler.fit_transform(clean_matrix_train.todense())\n",
    "# # X_test = scaler.fit_transform(clean_matrix_test.todense())\n",
    "# X_train = clean_matrix_train\n",
    "# X_test = clean_matrix_test\n",
    "y_train = df1.dropna()['overall'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9033b98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train = df_train[\"reviewText\"] + ' ' + df_train[\"summary\"]\n",
    "y_train = df_train['overall'].astype('int64')\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(text_train, y_train, test_size=0.3, random_state=1)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.5, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c976e8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=10000, oov_token='<UNK>')\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1eee51f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen=50\n",
    "sequences = tokenizer.texts_to_sequences(X_train)\n",
    "padded_train = pad_sequences(sequences, truncating = 'post', padding='post', maxlen=maxlen)\n",
    "\n",
    "classes = set(y_train)\n",
    "class_to_index = dict((c,i) for i, c in enumerate(classes))\n",
    "index_to_class = dict((v,k) for k, v in class_to_index.items())\n",
    "names_to_ids = lambda labels: np.array([class_to_index.get(x) for x in labels])\n",
    "train_labels = names_to_ids(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d917947",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(X_val)\n",
    "padded_val = pad_sequences(sequences, truncating = 'post', padding='post', maxlen=maxlen)\n",
    "\n",
    "classes = set(y_val)\n",
    "class_to_index = dict((c,i) for i, c in enumerate(classes))\n",
    "index_to_class = dict((v,k) for k, v in class_to_index.items())\n",
    "names_to_ids = lambda labels: np.array([class_to_index.get(x) for x in labels])\n",
    "val_labels = names_to_ids(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "929884da",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(X_test)\n",
    "padded_test = pad_sequences(sequences, truncating = 'post', padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4eac4e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    \n",
    "tf.keras.layers.Embedding(10000, 16, input_length=maxlen),\n",
    "tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(20, return_sequences=True)),\n",
    "tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(20, return_sequences=True)),\n",
    "tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(20)),\n",
    "tf.keras.layers.Dense(5, activation='softmax')\n",
    "    \n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b657ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "3281/3281 [==============================] - 64s 17ms/step - loss: 0.8088 - accuracy: 0.6892 - val_loss: 0.7336 - val_accuracy: 0.7100\n",
      "Epoch 2/20\n",
      "3281/3281 [==============================] - 53s 16ms/step - loss: 0.6994 - accuracy: 0.7230 - val_loss: 0.7319 - val_accuracy: 0.7099\n",
      "Epoch 3/20\n",
      "3281/3281 [==============================] - 54s 16ms/step - loss: 0.6597 - accuracy: 0.7375 - val_loss: 0.7215 - val_accuracy: 0.7138\n",
      "Epoch 4/20\n",
      "3281/3281 [==============================] - 55s 17ms/step - loss: 0.6193 - accuracy: 0.7529 - val_loss: 0.7416 - val_accuracy: 0.7062\n",
      "Epoch 5/20\n",
      "3281/3281 [==============================] - 55s 17ms/step - loss: 0.5735 - accuracy: 0.7729 - val_loss: 0.7743 - val_accuracy: 0.6998\n"
     ]
    }
   ],
   "source": [
    "h = model.fit(\n",
    "     padded_train, train_labels,\n",
    "     validation_data=(padded_val, val_labels),\n",
    "     epochs=20,\n",
    "     callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=2)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "526d30cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_probs = model.predict(padded_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dc56b862",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_predictions = []\n",
    "\n",
    "for preds in rnn_probs:\n",
    "    rnn_predictions.append(np.argmax(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874f73ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outfile = open('test_result.csv','w')\n",
    "# out = csv.writer(outfile)\n",
    "# out.writerows(map(lambda x: [x], rnn_predictions))\n",
    "# outfile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf-gpu] *",
   "language": "python",
   "name": "conda-env-tf-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
